{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 2,
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "kNoSpike = 100\n",
    "decay_rate = 1e-4 # random set, to fix\n",
    "decay_params={'rate':decay_rate, 'rate_inverse':1/decay_rate} \n",
    "layer_size = [784, 512, 10]\n",
    "weights = [np.zeros([512, 784]), np.zeros([10, 512])]\n",
    "fire_threshold=1.0\n",
    "M_E=1e5 # unknow yet\n",
    "#Minimum argument for the main branch of the Lambert W function.\n",
    "kMinLambertArg = -1.0 / M_E\n",
    "#Maximum argument for which gsl_sf_lambert_W0 produces a valid result.\n",
    "kMaxLambertArg = 1.7976131e+308\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNearBranchCutoff = -0.3235\n",
    "kE = 2.718281828459045\n",
    "def LambertW0InitialGuess(x):\n",
    "  # Sqrt approximation near branch cutoff.\n",
    "  if x < kNearBranchCutoff:\n",
    "    return -1.0 + sqrt(2.0 * (1 + kE * x))\n",
    "  # Taylor series between [-1/e and 1/e].\n",
    "  if x > kNearBranchCutoff and x < -kNearBranchCutoff:\n",
    "    return x * (1 + x * (-1 + x * (3.0 / 2.0 - 8.0 / 3.0 * x)))\n",
    "\n",
    "  # Series of piecewise linear approximations.\n",
    "  if x < 0.6:\n",
    "       return 0.23675531078855933 + (x - 0.3) * 0.5493610866617109;\n",
    "  if x < 0.8999999999999999:\n",
    "    return 0.4015636367870726 + (x - 0.6) * 0.4275644294878729;\n",
    "  if x < 1.2:\n",
    "    return 0.5298329656334344 + (x - 0.8999999999999999) * 0.3524368357714513;\n",
    "  if x < 1.5:\n",
    "    return 0.6355640163648698 + (x - 1.2) * 0.30099113800452154;\n",
    "  if x < 1.8:\n",
    "      return 0.7258613577662263 + (x - 1.5) * 0.2633490154764343;\n",
    "  if x < 2.0999999999999996:\n",
    "    return 0.8048660624091566 + (x - 1.8) * 0.2345089875713013;\n",
    "  if x < 2.4:\n",
    "    return 0.8752187586805469 + (x - 2.0999999999999996) * 0.2116494532726034;\n",
    "  if x < 2.6999999999999997:\n",
    "    return 0.938713594662328 + (x - 2.4) * 0.19305046534383152;\n",
    "  if x < 2.9999999999999996:\n",
    "    return 0.9966287342654774 + (x - 2.6999999999999997) * 0.17760053566187495;\n",
    "\n",
    "  # Asymptotic approximation.\n",
    "  l = log(x)\n",
    "  ll = log(l)\n",
    "  return l - ll + ll / l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kReciprocalE = 0.36787944117\n",
    "kDesiredAbsoluteDifference = 1e-3\n",
    "kNumMaxIters = 10\n",
    "\n",
    "def LambertW0(x):\n",
    "  if x <= -kReciprocalE:\n",
    "      return None, False\n",
    "  if x == 0.0:\n",
    "    return 0, True\n",
    "  if x == -kReciprocalE:\n",
    "    return -1.0, True\n",
    "\n",
    "  # Current guess.\n",
    "  w_n = LambertW0InitialGuess(x)\n",
    "  have_convergence = False\n",
    "\n",
    "  # Fritsch iteration.\n",
    "  for i in range(kNumMaxIters):\n",
    "    z_n = log(x / w_n) - w_n\n",
    "    q_n = 2.0 * (1.0 + w_n) * (1.0 + w_n + 2.0 / 3.0 * z_n)\n",
    "    e_n = (z_n / (1.0 + w_n)) * ((q_n - z_n) / (q_n - 2.0 * z_n))\n",
    "    w_n *= (1.0 + e_n)\n",
    "    # Done this way as the log is the expensive part above.\n",
    "    if abs(z_n) < kDesiredAbsoluteDifference:\n",
    "      have_convergence = True\n",
    "      break\n",
    "  return w_n, have_convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExponentiateSortedValidSpikes(activations, sorted_indices, decay_rate):\n",
    "  exp_activations = np.zeros_like(activations)\n",
    "  exp_activations.fill(kNoSpike)\n",
    "  i = 0\n",
    "  while i < len(sorted_indices) and activations[sorted_indices[i]] < kNoSpike:\n",
    "    exp_activations[sorted_indices[i]]  = np.exp(decay_rate * activations[sorted_indices[i]])\n",
    "    i += 1\n",
    "  return exp_activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ActivateNeuronAlpha(weight, activation, exp_activation, sorted_indices, threshold):\n",
    "  #causal_set, a, b, w, decay_params\n",
    "  A = 0.0\n",
    "  B = 0.0\n",
    "  W = 0.0\n",
    "  spike_time = kNoSpike\n",
    "  causal_set = np.zeros_like(activation)\n",
    "    # input spike one by one\n",
    "  for spike_idx in sorted_indices:\n",
    "    if spike_time <= activation[spike_idx]:\n",
    "        return spike_time # no need to integrate more\n",
    "        # Otherwise, integrate this spike\n",
    "        # Reset spike time, in case an inhibitory input cancels a potential spike.\n",
    "    spike_time = kNoSpike\n",
    "    causal_set[spike_idx] = 1\n",
    "\n",
    "    w_exp_z = weight[spike_idx] * exp_activation[spike_idx]\n",
    "    A += w_exp_z\n",
    "    B += w_exp_z * activation[spike_idx]\n",
    "\n",
    "# The value of the first derivative of the activation function in the\n",
    "# intersection point with the fire threshold is given by *A multiplied by a\n",
    "# never-negative value. Thus, if *A is negative the intersection will be in\n",
    "# a decreasing-potential area, and thus not a spike.\n",
    "    if A < 0:\n",
    "      continue\n",
    "    b_over_a = B/A\n",
    "    lambert_arg = -decay_params['rate'] * threshold / A * np.exp(decay_params['rate'] * b_over_a)\n",
    "    if lambert_arg >= kMinLambertArg and lambert_arg <= kMaxLambertArg:\n",
    "      val, convergence = LambertW0(lambert_arg)\n",
    "      assert convergence, \"Error computing Lambert W on: %f\" % (lambert_arg)\n",
    "      W = val\n",
    "      spike_time = b_over_a - W * decay_params.rate_inverse()\n",
    "\n",
    "      # For inhibitory weights, this might be a false alarm.\n",
    "      # This is not the same as spike_time < inputs[spike_ind]: it is also true for NaNs.\n",
    "      if not (spike_time >= activation[spike_idx]):\n",
    "          spike_time = kNoSpike\n",
    "    #END_IF\n",
    "  #END_FOR\n",
    "  # If we get here, either there is no spike, in which case\n",
    "  # all presynaptic neurons are to blame, or there is eventually\n",
    "  # a spike caused by all presynaptic inputs.\n",
    "  if (spike_time == kNoSpike):\n",
    "    causal_set=True\n",
    "  return spike_time, A,B,W,causal_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-5-38ecc1a7e5db>:27: RuntimeWarning: invalid value encountered in double_scalars\n  b_over_a = B/A\n<ipython-input-5-38ecc1a7e5db>:28: RuntimeWarning: divide by zero encountered in double_scalars\n  lambert_arg = -decay_params['rate'] * threshold / A * np.exp(decay_params['rate'] * b_over_a)\n"
     ]
    }
   ],
   "source": [
    "activation= np.zeros([784]) # input with batch\n",
    "for layer in range(len(layer_size)-1):\n",
    "    sorted_indices = np.argsort(activation)# sort indices from small to large\n",
    "    exp_activation = ExponentiateSortedValidSpikes(activation, sorted_indices, decay_rate)\n",
    "    activation_next = np.zeros(layer_size[layer+1])\n",
    "    A = np.zeros_like(activation_next)\n",
    "    B = np.zeros_like(activation_next)\n",
    "    W = np.zeros_like(activation_next)\n",
    "    causal_set = np.zeros([layer_size[layer+1], layer_size[layer]])\n",
    "    for n in range(layer_size[layer+1]):\n",
    "        act,a,b,w,c = ActivateNeuronAlpha(weights[layer][n], activation, exp_activation, sorted_indices, fire_threshold)\n",
    "        activation_next[n]=act \n",
    "        A[n]=a\n",
    "        B[n]=b\n",
    "        W[n]=w\n",
    "        causal_set[n]=c\n",
    "    activation=activation_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}